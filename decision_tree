
信息熵：
通俗解释：信息量的期望，表示随机变量的不确定性，度量样本集合的纯度，信息熵越小，样本集合纯度越高，确定性越大。
数学公式：H[x]=-∑p(x)log2p(x)

条件熵：在一个条件下，随机变量的不确定性。

信息增益：
通俗解释：在一个条件下，信息不确定性减少的程度！属性条件的信息增益越大，意味着使用此属性来进行划分所获得的“纯度”提升越大，一下子就变得确定了。
数学公式：熵 - 条件熵

举个例子：
通俗地讲，X(明天下雨)是一个随机变量，X的熵可以算出来， Y(明天阴天)也是随机变量，在阴天情况下下雨的信息熵我们如果也知道的话（此处需要知道其联合概率分布
或是通过数据估计）即是条件熵。两者相减就是信息增益！原来明天下雨例如信息熵是2，条件熵是0.01（因为如果是阴天就下雨的概率很大，信息就少了），
这样相减后为1.99，在获得阴天这个信息后，下雨信息不确定性减少了1.99！是很多的！所以信息增益大！也就是说，阴天这个信息对下雨来说是很重要的！
所以在特征选择的时候常常用信息增益，如果IG（信息增益大）的话那么这个特征对于分类来说很关键~~ 决策树就是这样来找特征的！



决策树算法：



决策树代码：

